(window.webpackJsonp=window.webpackJsonp||[]).push([[273],{732:function(e,t,r){"use strict";r.r(t);var n=r(1),a=Object(n.a)({},(function(){var e=this,t=e._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("h3",{attrs:{id:"摘要"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#摘要"}},[e._v("#")]),e._v(" 摘要")]),e._v(" "),t("p",[e._v("本文提出了CodeBert模型，一种可以同时用于编程语言PL和自然语言NL的双模态预训练模型，可以学习到通用的表征，用于下游的NL-PL应用，比如自然语言代码搜索，代码文本生成等等。CodeBert模型使用基于transformer的神经网络架构搭建而成，使用两种预训练任务，传统的MLM（掩码语言模型）和新加的RTD（替换token检测），其中RTD任务首先使用一个生成器预测句中被mask掉的"),t("a",{attrs:{href:"https://www.zhihu.com/search?q=token&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22476950957%22%7D",target:"_blank",rel:"noopener noreferrer"}},[e._v("token"),t("OutboundLink")],1),e._v("，接下来使用预测的token替代句中的[MASK]标记，然后使用一个判别器区分句中的每个token是原始的还是替换后的。实验结果表明，CodeBert模型在两个NL-PL下游任务（自然语言代码搜索，代码文本生成）中取得了SOTA的结果，本文还设计实验证明了CodeBert模型提取到的特征，相较于其他模型，对于NL-PL任务来说更有效果。")]),e._v(" "),t("h3",{attrs:{id:"介绍"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#介绍"}},[e._v("#")]),e._v(" 介绍：")]),e._v(" "),t("p",[e._v("大型预训练模型对几乎所有NLP任务都带来了显著改进，成功的方法是在大规模数据集上自监督的训练深度神经网络。比如GPT是根据上文的单词来预测下一个词，而Bert是根据上下文的单词来预测中间被mask掉的单词，以此更好的学习通用语言表征。")]),e._v(" "),t("p",[e._v("多模态预训练模型的关键是学习不同模态输入之间的隐形对齐，比如学习语言和图像的对齐。本文将自然语言和程序语言视为不同的模态，不仅学习包含NL-PL对的双模态数据，还学习大量的单模态数据，比如没有文档注释的代码，或者没有配对代码的文档注释。")]),e._v(" "),t("p",[e._v("CodeBert模型基于transformer架构，利用双模态NL-PL对数据和单模态PL数据来训练MLM和RTD两个与训练任务，数据集包含6种编程语言，其中编程语言类型不使用显示标记来标注。")]),e._v(" "),t("h3",{attrs:{id:"method"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#method"}},[e._v("#")]),e._v(" Method")]),e._v(" "),t("ul",[t("li",[e._v("模型结构")])]),e._v(" "),t("p",[e._v("CodeBert基于多层双向transformer网络，一共12层，每层12个Attention head，与RoBERTa的模型基本一致。")]),e._v(" "),t("ul",[t("li",[e._v("输入输出")])]),e._v(" "),t("p",[e._v("预训练阶段，输入部分将自然语言文本和代码文本之间使用[sep]间隔，构成{[cls], nl, [sep], pl, [eos]}的形式，输出部分包括每个token的embedding值和用于分类的[cls]值。")]),e._v(" "),t("ul",[t("li",[e._v("预训练数据")])]),e._v(" "),t("p",[e._v("成对的NL-PL双模态数据和仅有PL的单模态数据，具有6种不同的编程语言，下图是一个NL-PL对的数据样例，其中NL是下图中的红色部分，起到对函数的解释说明作用。")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://img-blog.csdnimg.cn/8425190a396340b188d36a02af670d6e.png",alt:"在这里插入图片描述"}})]),e._v(" "),t("ul",[t("li",[e._v("预训练任务")])]),e._v(" "),t("p",[e._v("本模型包括两个预训练任务，一个是MLM传统的Bert掩码语言模型，随机选择单词进行mask，然后来预测被mask的单词，这个任务使用双模态数据NL-PL对来进行预训练。")]),e._v(" "),t("p",[e._v("另一个预训练任务就是RTD任务了，使用PL和NL单模态数据来预训练，RTD替换token检测任务最早来源于ICLR2020的文章“"),t("a",{attrs:{href:"https://link.zhihu.com/?target=https%3A//openreview.net/pdf%3Fid%3Dr1xMH1BtvB",target:"_blank",rel:"noopener noreferrer"}},[e._v("ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators"),t("OutboundLink")],1),e._v("”，由于在Bert中传统的MLM任务随机选择句子内15%的token，其中80%被[MASK]替换，10%被随机替换，10%保持不变，随后将替换后的句子输入到BERT中用于预测那些被替换的token。")]),e._v(" "),t("p",[e._v("可是这样做Bert只学习这15%的token有点浪费算力，还存在[MASK]不会在实际任务中出现这种训练测试不一致问题，因此ELECTRA文章提出了RTD这个新的预训练任务，"),t("strong",[e._v("首先使用一个生成器预测句中被mask掉的token，接下来使用预测的token替代句中的[MASK]标记，然后使用一个判别器区分句中的每个token是原始的还是替换后的")]),e._v("。这种方式会让模型在所有token上学习，而不是仅仅学习那15%的token，因此计算效率更高.")]),e._v(" "),t("p",[e._v("回到本文，本文的RTD任务与ELECTRA的类似，两个生成器NL Generator和Code generator结合上下文来生成[MASK]处的单词，然后使用预测的单词对[MASK]位置进行替换，替换结束后使用NL-Code Discriminator来检查每一个单词是否是被替换的。")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://img-blog.csdnimg.cn/d6f655941301429d83dc82763f4337d5.png",alt:"在这里插入图片描述"}})]),e._v(" "),t("ul",[t("li",[e._v("Fine-tuning 阶段")])]),e._v(" "),t("p",[e._v("本文在自然语言代码搜索、代码文本生成这两个下游任务上进行fine-tuning，对于自然语言代码搜索任务，使用[cls]的表示来衡量自然语言查询与代码之间的语义相关性，对于代码文本生成任务，使用encoder-decoder框架，CodeBert模型用来初始化encoder部分。")]),e._v(" "),t("h3",{attrs:{id:"experiment"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#experiment"}},[e._v("#")]),e._v(" Experiment")]),e._v(" "),t("ul",[t("li",[e._v("自然语言代码搜索")])]),e._v(" "),t("p",[e._v("该任务是输入一个自然语言查询，找到与其语义最相关的代码，在CodeSearchNet数据集上进行训练，数据集中包括6种编程语言，数据集统计信息如下。")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://img-blog.csdnimg.cn/13ced19a18d3429497966e5157a89f76.png",alt:"在这里插入图片描述"}})]),e._v(" "),t("p",[e._v("下图展示了不同的方法在CodeSearchNet数据集上的实验结果：")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://img-blog.csdnimg.cn/01448a90a02042489ecdc9b827f1a403.png",alt:"在这里插入图片描述"}})]),e._v(" "),t("p",[e._v("第一组是传统的神经网络模型，第二组是RoBERTa模型，其中PT的预训练的意思，w/ Code only表示只使用代码进行预训练(w/o表示没有)，第三组是本文的CodeBert模型，分别比较了使用不同预训练任务的结果，可见同时使用MLM和RTD的结果是最好的。其中INIT=s的含义是从零开始训练，INIT=R的含义是使用RoBERTa模型的预训练参数初始化模型进行训练，可见，使用RoBERTa参数初始化的效果更好。")]),e._v(" "),t("ul",[t("li",[e._v("代码文本生成")])]),e._v(" "),t("p",[e._v("代码文本生成就是给定一段代码，生成一段描述该段代码语义信息的文本，下图展示了各种方法在CodeSearchNet数据集上的实验结果：")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://img-blog.csdnimg.cn/e98d65d049c04bce87e793fc1183b46f.png",alt:"在这里插入图片描述"}})]),e._v(" "),t("ul",[t("li",[e._v("NL-PL Probing（直接看预训练效果）")])]),e._v(" "),t("p",[e._v("本文除了在2个fine-tuning任务上进行验证外，还探究了CodeBert模型具体学习到了哪些类型的知识，给定一个NL-PL对，Probing实验的目标是在一些干扰选项之间选择正确的masked token，具体是设定一些cloze-style的问题，即使用[MASK]替代某个token，然后让模型选择正确的选项，其中干扰项是根据专业知识选取出来的。")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://img-blog.csdnimg.cn/6f51306bdbf44044b805e87bc7486dae.png",alt:"在这里插入图片描述"}})]),e._v(" "),t("p",[e._v('上图是一个针对python语言的probing的例子，对于NL（蓝色）和PL（黄色），分别mask掉单词"min"，然后给出{max, min, less, greater} 4个选项去做选择，结果显示Boberta做出了错误的选择，而CodeBert则选择正确。')]),e._v(" "),t("p",[e._v("下图为Probing任务在整个数据集上的整体结果，可见对于PL任务会给出2个选择，而NL任务会给出4个选择，NL任务会输入带mask的NL和全部PL，PL任务会输入带mask的PL和全部NL（PL任务增加了一种情况，输入mask之前的PL部分和全部NL，称为precede-PL），在PL，precede-PL，和NL任务上，基于MLM的CodeBert模型均取得了最好的结果。")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://img-blog.csdnimg.cn/a0fa933effd246eaaa3e56e423c9ce4d.png",alt:"在这里插入图片描述"}})]),e._v(" "),t("h3",{attrs:{id:"conclusion"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#conclusion"}},[e._v("#")]),e._v(" Conclusion")]),e._v(" "),t("p",[e._v("本文提出了第一个同时针对NL和PL的大型双模态预训练模型CodeBert，在双模态和单模特数据上训练CodeBert，并在两个NL-PL下游任务中进行fine-tuning取得了SOTA的结果，接下来的工作目标可以放在使用更好的神经网络架构，或将AST等结构融入特征提取的过程中。")])])}),[],!1,null,null,null);t.default=a.exports}}]);