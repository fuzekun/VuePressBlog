(window.webpackJsonp=window.webpackJsonp||[]).push([[271],{735:function(e,a,t){"use strict";t.r(a);var v=t(1),i=Object(v.a)({},(function(){var e=this,a=e._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("h3",{attrs:{id:"vuldeelocator-a-deep-learning-based-fine-grained-vulnerability-detector-tdsc-a-2021-。"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#vuldeelocator-a-deep-learning-based-fine-grained-vulnerability-detector-tdsc-a-2021-。"}},[e._v("#")]),e._v(" VulDeeLocator: A Deep Learning-based Fine-grained Vulnerability Detector，TDSC(A) 2021 。")]),e._v(" "),a("p",[e._v("本文提出一个针对C语言的新的漏洞检测模型"),a("code",[e._v("VulDeeLocator")]),e._v("，将同一工程下的多个文件通过define-use的关系连接起来，并用中间代码表示替换源代码表示。将"),a("code",[e._v("BRNN")]),e._v("模型扩展为"),a("code",[e._v("BRNN-vdl")]),e._v("模型，不仅能够检测漏洞，还能够将漏洞出现的位置限制在几行之内。本文提出了一个新的数据集，每个样例都由"),a("code",[e._v("LLVM")]),e._v("的中间代码和源代码一同构成，在合成数据集和真实数据集上都验证了"),a("code",[e._v("VulDeeLocator")]),e._v("的有效性")]),e._v(" "),a("ul",[a("li",[a("p",[e._v("跨语言模型，单双模态进行漏洞检测")])]),e._v(" "),a("li",[a("p",[a("code",[e._v("GNN")]),e._v("表示")])]),e._v(" "),a("li",[a("p",[e._v("输出不仅是"),a("code",[e._v("mlp")]),e._v("，也可以定位到行")])])]),e._v(" "),a("p",[e._v("sudo vi /etc/hosts")]),e._v(" "),a("p",[e._v("104.244.46.208  github.global.ssl.fastly.net")]),e._v(" "),a("p",[e._v("108.160.170.39  assets-cdn.github.com")]),e._v(" "),a("p",[e._v("service networking restart")]),e._v(" "),a("h4",{attrs:{id:"摘要"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#摘要"}},[e._v("#")]),e._v(" 摘要")]),e._v(" "),a("p",[e._v("编程语言的预训练模型最近在代码智能方面取得了巨大成功。为了支持与代码相关的理解和生成任务，最近的工作尝试预训练统一的编码器-解码器模型。然而，这种编码器-解码器框架对于自回归任务来说是次优的，尤其是对于需要解码器才能进行有效推理的代码完成。在本文中，我们提出了UniXcoder，一种用于编程语言的统一跨模态预训练模型。该模型利用带有前缀适配器的掩码注意矩阵来控制模型的行为，并利用AST和代码注释等跨模式内容来增强代码表示。为了对表示为树的AST进行并行编码，我们提出了一种一对一映射方法，将AST转换为保留树中所有结构信息的序列结构。此外，我们建议利用多模态内容通过对比学习学习代码片段的表示，然后使用跨模态生成任务在编程语言之间对齐表示。我们在九个数据集上评估UniXcoder的五个代码相关任务。为了进一步评估代码片段表示的性能，我们还构建了一个新任务的数据集，称为零镜头代码到代码搜索。结果表明，我们的模型在大多数任务中都达到了最先进的性能，分析表明，注释和AST都可以增强UniXcoder。")]),e._v(" "),a("h3",{attrs:{id:"介绍"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#介绍"}},[e._v("#")]),e._v(" 介绍")]),e._v(" "),a("p",[e._v("预训练模型，如GPT（Radford等人，2018）和BERT（Devlin等人，2018年），在许多自然语言处理（NLP）任务中大大提高了技术水平。")]),e._v(" "),a("p",[e._v("这些预先训练的模型是预先训练的具有自我监督目标的大量文本数据，并且可以进行微调以适应下游任务。受NLP中预训练模型的成功启发，已经提出了用于编程语言（PL）的预训练模型（Kanade等人，2019；Feng等人，2020；Svyatkovskiy等人，2020），以促进代码智能的发展。Svyatkovskiy等人（2020）提出了GPT-C，该GPT-C采用从左到右的变压器（V aswani等人，2017）来支持生成任务，如代码完成，但单向框架对于理解任务来说是次优的。相比之下，其他工作（Kanade等人，2019；Feng等人，2020）在源代码上预训练了双向Transformer编码器，这显著提高了代码相关理解任务的性能。")]),e._v(" "),a("p",[e._v("然而，当应用于生成任务时，它的双向性需要额外的解码器，在这种情况下，解码器从头开始初始化，无法从预训练中受益。")]),e._v(" "),a("p",[e._v("在这项工作中，我们介绍了UniXcoder，这是一种统一的跨模态预训练编程语言模型，用于支持与代码相关的理解和生成任务。UniXcoder基于多层Transformer，遵循Dong等人。  利用带有前缀适配器的掩码注意矩阵来控制对每个令牌的上下文的访问。在代码智能方面，与当前的统一编码器模型（Ahmad等人，2021；Wang等人，2021）相比，UniXcoder可以更好地应用于自动回归任务，例如代码完成，在实践中需要仅使用解码器的方式来执行有效的推理。除了将代码作为唯一的输入，我们还考虑了多模式内容，如代码注释和抽象语法树（AST），以增强代码表示。通常，用户编写的代码注释提供关于源代码的关键语义信息，如“排序给定列表”，AST包含丰富的语法信息，如语句类型和语句之间的嵌套关系，这有助于  模型更好地理解源代码。为了对表示为树的AST进行并行编码，我们提出了一种一对一映射方法，将AST转换为保留树的所有信息的序列结构，然后将序列用作输入以增强代码表示。")]),e._v(" "),a("p",[e._v("我们使用三种类型的语言建模任务预训练UniXcoder：掩蔽语言建模（Devlin等人，2018）、单向语言建模（Radford等人，2018年）和去噪目标（Raffel等人，2019年），这可以使模型支持各种类型的下游任务。此外，我们引入了两个预训练任务来学习可以表示代码片段语义的嵌入。一种是利用AST增强代码片段嵌入语义的多模态对比学习，另一种是跨模态生成，利用代码注释在编程语言之间对齐嵌入。")]),e._v(" "),a("p",[e._v("我们在九个公共数据集上评估UniXcoder的五个任务，包括两个理解任务：克隆检测和代码搜索，两个生成任务：代码摘要和代码生成，以及一个自回归任务：代码完成。为了进一步测试代码片段嵌入，我们提出了一项新任务，称为零镜头代码到代码搜索，并从CodeNet语料库（Puri等人，2021）中为此任务构建了一个新的数据集。实验结果表明，我们的模型在大多数任务中都达到了最先进的性能。进一步分析表明，AST和代码注释都可以增强UniXcoder以更好地捕获代码语义。")]),e._v(" "),a("p",[e._v("总之，本文的贡献是：（1）我们提出了一个利用多模态内容的统一跨模态预训练模型，即。代码注释和AST，以支持代码相关的理解、生成任务和自动回归任务。（2） 我们提出了一个一对一映射函数，它将AST转换为保留AST所有信息的序列，并可以用源代码和注释并行编码。（3） 我们进一步建议利用代码注释来学习代码片段表示，并构建一个新的零镜头代码搜索数据集，以评估代码片段表示的质量。（4） 实验结果表明，UniXcoder在大多数下游任务上都有显著改进。")]),e._v(" "),a("h3",{attrs:{id:"相关工作"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#相关工作"}},[e._v("#")]),e._v(" 相关工作")]),e._v(" "),a("p",[e._v("随着自然语言（NL）处理中预训练的巨大成功（Devlin等人，2018；Lewis等人，2019；Raffel等人，2018；Brown等人，2020），已经提出了编程语言的预训练模型，以促进代码智能的发展。这些预训练模型通常可分为三类：编码器模型、仅解码器模型和编码器-解码器模型。")]),e._v(" "),a("p",[e._v("仅编码模型（Kanade等人，2019；Buratti等人，2020；Feng等人，2020年；Guo等人，2020，Wang等人，2022）预训练双向变压器，其中每个令牌可以相互关注。")]),e._v(" "),a("p",[e._v("Kanade等人（2019）通过掩蔽语言建模和下一句预测目标，在Python源代码语料库上预训练CuBERT。")]),e._v(" "),a("p",[e._v("CodeBERT（Feng et al.，2020）在六种编程语言中的NLPL对上进行了预训练，并具有新的预训练任务，即替换令牌检测。")]),e._v(" "),a("p",[e._v("GraphCodeBERT（Guo等人，2020）利用数据流增强代码表示，而SYNCOBERT（Wang等人，2022）通过AST边缘预测和对比学习结合了抽象语法树。然而，仅编码器的模型需要额外的解码器来执行生成任务，在这种情况下，解码器从头开始初始化，无法从预训练中受益。")]),e._v(" "),a("p",[e._v("至于仅限解码器的预训练模型，Sviatkovskiy等人（2020）和Lu等人（2021）分别提出了GPT-C和CodeGPT，这两种模型都是使用单向语言建模进行预训练的，只允许令牌参与之前的令牌，并允许令牌本身预测下一个令牌。")]),e._v(" "),a("p",[e._v("只有解码器的模型擅长于像代码完成这样的自回归任务，但单向框架对于理解任务来说是次优的。")]),e._v(" "),a("p",[e._v("最近的一些工作探索了编码器-解码器模型，以支持理解和生成任务。PLBART（Ahmad等人，2021）基于BART（Lewis等人，2019）架构，并使用去噪目标对NL和PL语料库进行预训练。CodeT5（Wang等人，2021）采用了T5（Raffel等人，2019）模型，该模型考虑了来自标识符的关键令牌类型信息，并允许对下游任务进行多任务学习。")]),e._v(" "),a("p",[e._v("TreeBERT（姜等人，2021）遵循编码器-编码器-变压器框架，但通过建模AST路径来利用树结构信息。")]),e._v(" "),a("p",[e._v("与当前的统一模型不同，UniXcoder基于多层Transformer，并使用带有前缀适配器的掩码注意矩阵 以控制模型的行为以支持理解和生成任务。与编码器-解码器架构相比，UniXcoder可以更好地应用于自回归任务，如IDE中广泛使用的代码完成，因为该任务需要仅解码器的方式来执行有效的推理。Liu等人。")]),e._v(" "),a("p",[e._v("（2020）还用多任务学习预训练了类似的模型CugLM，但它们只关注代码完成，而不是各种任务。此外，我们通过一对一映射函数将AST中的语法信息转换为序列，以增强代码表示。与以前使用AST的预训练模型不同，映射函数保留了来自AST的所有结构信息，并且不需要额外的预训练任务（例如边缘预测）来隐式学习AST结构。")]),e._v(" "),a("h3",{attrs:{id:"unixcoder"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#unixcoder"}},[e._v("#")]),e._v(" UniXcoder")]),e._v(" "),a("p",[e._v("在本节中，我们描述了UniXcoder，一种统一的跨模态预训练模型，它利用多模态数据（即代码注释和AST）来预训练代码表示。该模型基于Transformer，并使用带有前缀适配器的掩码注意矩阵（Dong等人，2019）来控制模型的行为。在下文中，我们首先介绍如何将多模态数据统一为UniXcoder的输入（§3.1），然后介绍模型架构（§3.2）和预训练任务")]),e._v(" "),a("h4",{attrs:{id:"input-representation"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#input-representation"}},[e._v("#")]),e._v(" Input Representation")]),e._v(" "),a("p",[e._v("我们在图1中给出了一个带有注释和AST的python代码示例 可以看到，注释“返回数据的样本算术平均值”高度描述了源代码的功能，它提供了有关源代码的关键语义信息。此外，AST提供了丰富的语法信息，例如子树“参数→ “（数据）”表示函数定义中术语（数据）的类型（即参数）。它们都可以用作附加知识，以增强预训练模型中的代码表示。然而，AST通常表示为树，不能直接用作Transformer的输入。为了将AST与代码注释并行编码，我们提出了算法1中描述的一对一映射函数F，以将AST转换为保留所有结构信息的序列。")]),e._v(" "),a("p",[e._v("特别地，给定AST的根节点根，该算法递归地将相同的函数F应用于其子节点，然后在其两侧添加两个特殊后缀（分别为lef t和right）（算法1的第6-11行）。如果根节点是叶，我们直接生成它的名称（第45行）。取“参数→ 例如，映射函数F将子树转换为“＜parameters，left＞（数据）＜parameter，right＞”。 可以有多种方法将树转换为标记序列，例如预排序遍历。")]),e._v(" "),a("p",[e._v("然而，特定的转换应该是一对一映射函数。否则，映射可能会将树与另一个结构混淆。我们的映射函数F满足这一要求（见附录A的证明）。最后，给定源代码C，我们将其注释W={w0，w1，…，wm−1} 和平坦的AST 标记序列F（T（C））={c0，c1，…，ck−1} 其中T（C）是代码的AST的根。对于输入格式，我们将它们与一个前缀连接起来作为输入序列，如图2底部所示，其中前缀表示模型的工作模式，下面将讨论")]),e._v(" "),a("h4",{attrs:{id:"model-architecture"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#model-architecture"}},[e._v("#")]),e._v(" Model Architecture")]),e._v(" "),a("p",[e._v("图2显示了UniXcoder的模型架构。该模型在代码注释上应用N个变换器层，并用前缀平坦化AST，以产生隐藏状态HN={hN0，hN1，…，hNn−1} ，其中前缀p∈ {[Enc]、[Dec]、[22D]}表示模型的行为，例如[E2D]表示UniXcoder作为编码器-解码器模型工作。")]),e._v(" "),a("p",[e._v("每个变压器层包含一个结构相同的变压器，该变压器使用多头自关注操作（V aswani et al.，2017），然后在前一层的输出上使用前馈层。对于第l个变压器层，多头自关注的输出通过 其中前一层的输出Hl−1.∈ Rn×dh分别线性映射到查询、键和值的三元组。dk是头部的尺寸，M∈ Rn×n是一个掩码矩阵，用于控制令牌在计算其上下文表示时可以处理的上下文，如图2中间所示。如果允许第i个令牌处理第j个令牌，则Mij设置为0，否则−∞")]),e._v(" "),a("p",[e._v("对于仅编码器模式，我们在输入前面添加一个特殊标记[Enc]作为前缀，并将掩码矩阵的所有元素设置为0，以允许所有标记相互关注。对于仅解码器模式，使用前缀[Dec]，掩码的上三角部分设置为−∞ 以指示每个令牌只能关注自身和先前令牌。对于编码器-解码器模式，允许源输入中的令牌相互关注，而目标输入中的符号只关注自身以及源和目标输入中之前的令牌。我们使用[E2D]前缀来表示UniXcoder作为编码器-解码器模型工作。在预训练阶段，模型参数以不同的模式共享，并根据多个目标进行优化，以支持各种类型的下游任务,")]),e._v(" "),a("h3",{attrs:{id:"pre-training-tasks"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#pre-training-tasks"}},[e._v("#")]),e._v(" Pre-training Tasks")]),e._v(" "),a("p",[e._v("我们将在本节中描述UniXcoder中使用的预训练任务。如图2右侧所示，我们首先使用三个任务预训练UniXcoder，包括掩蔽语言建模（Devlin等人，2018）、单向语言建模（Radford等人，2018年）和去噪目标（Raffel等人，2019年）。这些任务针对不同的模式设计，使UniXcoder能够支持各种类型的与代码相关的下游任务。然后，我们建议利用多模态数据通过对比学习和跨模态生成来学习代码片段嵌入，如图3所示。")]),e._v(" "),a("p",[e._v("掩码语言建模对于仅编码器模式，我们遵循Devlin等人（2018）应用掩码语言建模（MLM）预训练任务。特别地，我们从输入序列中抽取15%的标记Sm，然后用[MASK]（随机）标记替换其中的80%（10%），并保留另外10%的标记不变。任务是根据双向上下文标记预测掩码标记的原始标记，如图2（a）所示。特别地，模型可以利用来自注释的语义信息和来自AST的语法信息来推断屏蔽代码标记，这鼓励模型从不同的知识资源学习代码表示。目标按等式3计算，其中Xmask是屏蔽的输入序列.")]),e._v(" "),a("p",[e._v("单向语言建模我们使用单向语言建模（ULM）预训练任务来预训练仅解码器模式，以支持代码完成等自回归任务，如图2（b）所示。该任务根据之前的令牌和自身{x0，x1，…，xi逐一预测下一个令牌xi−1} ，这可以使用三角形矩阵进行关注掩模。")]),e._v(" "),a("p",[e._v("去噪目标去噪（DNS）预训练目标已被证明对NLP中的BART（Lewis et al.，2019）和T5（Raffel et al.）等编码器-解码器模型非常有效。该任务随机屏蔽任意长度的跨度，然后在编码器-编码器模式下生成这些屏蔽的跨度。更好地支持生成任务.")]),e._v(" "),a("p",[e._v("与代码摘要一样，我们对编码器-解码器模式使用与T5相似的去噪目标，如图2（c）所示。特别地，我们首先将输入序列分成最大（bn×rl c，1）个块，然后为每个块随机屏蔽1到2l-1个标记的跨度，其中n是输入的长度，r是损坏率，l是屏蔽跨度的平均长度。我们分别将腐败率设置为15%，平均长度设置为5。连接{y0，y1，…，yn−1} 在第k个跨距前面带有特殊标记[M ASKk]的所有掩码跨距中，将用作输出：")]),e._v(" "),a("p",[e._v("代码片段表示学习:")]),e._v(" "),a("p",[e._v("除了针对不同模式设计的上述三个预训练任务之外，我们建议利用多模式数据来学习代码片段Ci的语义嵌入ehi。如图3所示，我们首先使用UniXcoder对映射的AST序列进行编码，然后在源输入的隐藏状态上应用平均池层，以获得语义嵌入ehi。为了学习语义嵌入，我们提出了两个预训练任务。")]),e._v(" "),a("p",[e._v("一种是多模态对比学习（MCL），另一种是跨模态生成（CMG）。 对于多模式对比学习，我们遵循Gao等人（2021）使用不同的隐藏退出掩码转发相同的输入作为正例eh+i，并使用同一批次中的其他表示作为反例。损失按等式6计算，其中b为批量，τ为温度超参数，cos（·，·）为两个向量之间的余弦相似性。")]),e._v(" "),a("p",[e._v("对于跨模态生成，我们要求模型生成其注释W={w0，w1，…，wm−1}.")]),e._v(" "),a("p",[e._v("注释描述了代码的功能，这不仅可以帮助模型理解代码语义，还可以通过统一的自然语言描述作为支点，在不同的编程语言之间对齐表示。由于注释的生成取决于 代码，它将迫使模型将注释中的语义信息融合到代码的隐藏状态中。损失按等式7计算，其中X是平坦的AST令牌序列 .")]),e._v(" "),a("p",[e._v("为了学习自然语言的语义嵌入，我们以50%的概率随机交换源输入和目标输入.考虑到在下游任务中显式添加AST将带来额外的成本，如解析时间和增加输入长度（标记化后输入长度增加70%），我们通过预训练隐式地从AST学习知识，并仅在微调阶段保留AST的叶子（即源代码）。可以通过在预训练阶段以50%的概率随机丢弃AST的所有非终端符号来缓解该差距。有关预训练数据集和设置的更多详细信息.")]),e._v(" "),a("h3",{attrs:{id:"experiments"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#experiments"}},[e._v("#")]),e._v(" Experiments")]),e._v(" "),a("p",[e._v("我们在九个公共数据集上对UniXcoder进行了五项任务评估，包括两项理解任务（§4.2）、两项生成任务（§4.3）和一项自回归任务（§4.4）。为了进一步评估代码片段嵌入的性能，我们还提出了一个新任务，称为零炮代码到代码搜索（§4.5）。有关数据集和微调的更多详细信息，请参见附录C")]),e._v(" "),a("p",[e._v("我们将UniXcoder与最先进的预训练模型进行了比较，包括仅编码器、解码器和编码器-解码器模型")]),e._v(" "),a("p",[e._v("对于仅编码器的模型，我们考虑了Roberta（Liu等人，2019）用MLM在文本语料库上预训练，CodeBERT（Feng等人，2020）在NL-PL对上使用MLM和替换标记检测预训练，GraphCodeBERT（Guo等人，2020年）利用数据流增强代码表示，SYNCOBERT结合了AST边缘预测和对比学习")]),e._v(" "),a("p",[e._v("对于纯解码器模型，我们考虑了GPT-2（Radford等人，2019）和CodeGPT（Lu等人，2021），其中前者在文本语料库上预训练，后者在CodeSearchNet数据集上预训练。两者都使用ULM作为目标。")]),e._v(" "),a("p",[a("strong",[e._v("UniXcoder: 统一的跨模式预训练模型")])]),e._v(" "),a("p",[e._v("Unixcoder是一种用于编程语言的统一的跨模式预训练模型。该模型利用带有前缀适配器的掩码注意矩阵来控制模型的行为，并利用AST和代码注释等跨模式内容来增强代码表示。为了对并行表示为树的AST进行编码，论文提出了一种一对一的映射方法，可以保留AST中所有结构信息的序列结构。该模型还利用多模态内容通过对比学习来学习代码片段的表示，然后使用跨模态生成任务来对齐编程语言之间的表示。")]),e._v(" "),a("h1",{attrs:{id:"vdsimilar-vulnerability-detection-based-on-code-similarity-of-vulnerabilities-and-patches"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#vdsimilar-vulnerability-detection-based-on-code-similarity-of-vulnerabilities-and-patches"}},[e._v("#")]),e._v(" VDSimilar: Vulnerability detection based on code similarity of vulnerabilities and patches")]),e._v(" "),a("h2",{attrs:{id:"来源信息"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#来源信息"}},[e._v("#")]),e._v(" 来源信息")]),e._v(" "),a("ul",[a("li",[e._v("机构：中国科学院大学、山东师范大学、广西师范大学")]),e._v(" "),a("li",[e._v("作者：Hao Suna, Lei Cui, Lun Li, Zhenquan Ding, Zhiyu Hao,Jiancong Cui, Peng Liu")]),e._v(" "),a("li",[e._v("期刊：Computers & security（CCF B）")])]),e._v(" "),a("h3",{attrs:{id:"abstract"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#abstract"}},[e._v("#")]),e._v(" Abstract")]),e._v(" "),a("p",[e._v("现有的研究将漏洞检测视为一个分类问题，在捕获语义和语法相似性的同时需要大量的标记数据。本研究认为漏洞的相似性是漏洞检测的关键，本文准备了一个由漏洞和相关补丁组成的相对较小的数据集，并尝试比较漏洞之间的相似性、漏洞补丁之间的差异性来实现漏洞检测。为此，使用Siamese网络+BiLSTM+Attention作为检测模型。在OpenSSL和Linux的876个漏洞和补丁的数据集上，提出了模型VDSimilar，在OpenSSL的AUC值上达到了约97.17%，优于目前基于深度学习的漏洞检测SOTA方法。")]),e._v(" "),a("h3",{attrs:{id:"introduction"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#introduction"}},[e._v("#")]),e._v(" Introduction")]),e._v(" "),a("p",[e._v("现有的基于代码相似性的漏洞检测方法普遍是基于代码段语法和语义的相似性来进行的，但是两份语法语义相似的代码很可能因为一丁点差别而一个有漏洞一个没有漏洞，因此，本文希望找到一个能从漏洞角度捕获相似性的方法。另外，基于深度学习的方法总是需要大量的数据，比如VulDeepecker需要61638个code gadget，准备这样的数据集需要花费巨大的人力资源，本文希望找到一个可以在小数据集上使用的深度学习检测方法。")]),e._v(" "),a("p",[e._v("本文提出了一个基于度量学习的代码漏洞检测方法，学习了一个应用于漏洞和补丁数据集上的代码相似性检测器。首先，准备一个CVE brunch的数据集，每个CVE brunch包含与一个CVE相关的多个漏洞函数和补丁函数，如上图所示，这些CVE函数可以从不同版本的软件中获得，它们遵循两个规则：")]),e._v(" "),a("ul",[a("li",[e._v("对于同一个CVE中两个版本的漏洞函数，漏洞片段保持不变")]),e._v(" "),a("li",[e._v("对于一个漏洞函数和一个补丁函数，漏洞片段一定消失")])]),e._v(" "),a("p",[e._v("因此，每个CVE brunch都可能提供一个CVE漏洞特征。其次，从漏洞的角度来看，不同版本的两个漏洞函数应该被视为相似的，即使版本迭代过程中存在代码更改；另一方面，由于补丁代码中漏洞片段已经消除，因此即使漏洞函数与补丁函数语法和语义相似，也应视为不同。本研究在本文提出的数据集上与之前的方法比较，证明了VDSimilar的有效性。")]),e._v(" "),a("p",[e._v("本文认为，相比整个漏洞函数，漏洞代码片段是漏洞检测的关键，如下图所示，显示了t1_lib.c 的 tls_decrypt_ticket函数的代码片段，该函数在OpenSSL的三个版本中演进。该功能被报告为一个漏洞(CVE-2014-3567)，影响0.9.8zb和1.0.1i，然后在更高版本的1.0.1l中修复，下图中第二个函数应该是1.0.1i。")]),e._v(" "),a("p",[e._v("可见，相比第一版，第二版增加了4行，改变了一行，都是漏洞函数。第三版相比第二版只增加了一行，和第一版相比更接近于第二版，但第三版确是补丁函数。OpenSSL程序不断发展，其中一个函数可能会由于诸多原因修改，如修复bug、优化性能或重构代码。两个版本的相同函数在语法和语义上可能会有很大的差异，而对于需要修复的漏洞，补丁可能只涉及几行甚至一行代码，因此跨连续版本的漏洞函数和补丁函数在语法上是高度相似的。")]),e._v(" "),a("p",[e._v("因此，一种好的基于代码相似度的漏洞检测方法应该更多地关注漏洞片段的相似度，而不是整个函数的代码语法和语义的相似度。")]),e._v(" "),a("h3",{attrs:{id:"method"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#method"}},[e._v("#")]),e._v(" Method")]),e._v(" "),a("p",[e._v("上图是VDSimilar的整体framework，数据集包含了很多的CVE漏洞，每个CVE漏洞由几个版本的漏洞函数和补丁函数组成。由于训练样本过少，本文采用度量学习的方式训练计算相似性的分类器，对于漏洞-漏洞函数对，标记为相似label为1，对于漏洞-补丁函数对，标记为不相似label为0，训练Siamese网络，训练过程中引入Attention，最终计算测试函数和漏洞库函数的相似性，相似性高于一定阈值被认为存在漏洞")]),e._v(" "),a("p",[a("strong",[e._v("数据准备")])]),e._v(" "),a("p",[e._v("为了准备数据集，本文从CVE Details数据库中收集了一组CVE，如下图所示，漏洞一般可以通过两种途径得到。一种是直接下载product，然后根据漏洞详细信息中描述的文件名和函数名提取漏洞函数；另一种是直接从外部链接中引用的补丁中提取漏洞；由于有些外部链接并不可用，因此本文采用第一种方式。")]),e._v(" "),a("p",[e._v("为了获得补丁，假设最新的有漏洞版本之后的Linux和OpenSSL版本漏洞已经修复，通过从多个版本的程序中提取漏洞和补丁函数，生成一组相似对({V, V, 1})和差异对({V, P, 0})。使用网络爬虫Scrapy框架爬取CVE Details中的程序版本、漏洞函数名、文件名和补丁等等信息，可以为每个CVE获取一个元组，即(CVE、软件、漏洞版本、补丁版本、文件名、函数名)。")]),e._v(" "),a("p",[e._v("对于每一个CVE，根据上文提取的详细信息，使用LLVM解析源代码，提取出一组漏洞函数和补丁函数。使用hash的方式去除重复函数、修正可能出现的漏洞标签错误信息。然后生成相似对和差异对，在该过程中扩大数据集，方便训练。")]),e._v(" "),a("ul",[a("li",[a("strong",[e._v("检测模型")])])]),e._v(" "),a("p",[e._v("本文的检测模型Siamese架构如上图所示，首先进行embedding，然后输入BiLSTM中得到输出，经过一层Attention后计算相似度即可。值得一提的是该Attention是self-attention，类似transformer一样，由H成参数矩阵生成Q,K,V，然后进行Attention计算，该Attention过程的作用是将注意力聚集在漏洞代码片段而不是整个函数上。")]),e._v(" "),a("p",[e._v("Siamese网络是一个共享权重的孪生网络，模型训练过程中最大化漏洞函数之间的相似度，最小化漏洞函数和补丁函数的相似度。在测试过程中计算每个目标函数与已有漏洞函数的相似度，如果接近1则有漏洞，如果接近0则没有漏洞，算法伪代码如下。")]),e._v(" "),a("h3",{attrs:{id:"evaluation"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#evaluation"}},[e._v("#")]),e._v(" Evaluation")]),e._v(" "),a("p",[e._v("与Simian，Nicad，ReDeBug，PMD-CPD，SyseVR，VulDeePecker这几个方法做比较，本文的Siamese模型在Linux和OpenSSL数据集上取得了更高的检测准确率和F1值。")]),e._v(" "),a("h3",{attrs:{id:"结论"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#结论"}},[e._v("#")]),e._v(" 结论：")]),e._v(" "),a("p",[e._v("本文提出了一个基于代码相似性的源代码漏洞检测方法，准备了一个由漏洞和相关补丁组成的相对较小的数据集，并尝试比较漏洞之间的相似性、漏洞补丁之间的差异性来实现漏洞检测。为此，使用Siamese网络+BiLSTM+Attention作为检测模型。在OpenSSL和Linux的876个漏洞和补丁的数据集上取得了良好的实验效果，证明了模型的有效性.")]),e._v(" "),a("p",[e._v("准备了一个由漏洞和相关补丁组成的相对较小的数据集，并试图通过（i）漏洞对之间的相似性和（ii）漏洞对和补丁之间的差异来实现安全相似性")]),e._v(" "),a("p",[e._v("例如内存缓冲区溢出、输入验证不正确、越界写入。代码相似性的基本思想是比较当前已知的漏洞和测试程序代码，以找到匹配项。")]),e._v(" "),a("p",[e._v("当前基于代码相似性的方法倾向于检测在语义和语法上与已知漏洞相似的漏洞。然而，语义和句法相似性并不能保证找到漏洞。这是因为易受攻击的代码段通常包含整个易受攻击函数的一个小片段，也就是说，该代码段可能只包含几行甚至一行代码，而该函数包含几十到几百行代码。因此，对于在语法和语义上可能相同的两个函数，即使是细微的差异也可能导致从漏洞的角度来看的不同类别（即，一个是脆弱的，另一个是良性的），如我们将在图2中详细描述的，而不仅仅是语法和语义.")]),e._v(" "),a("p",[e._v("我们在暹罗模型中加入了BiLSTM和注意力网络，这有助于生成更准确、更集中的检测函数表示。通过这种方式，训练后的模型可以从脆弱性的角度感知相似性，而不仅仅是语义和语法。")]),e._v(" "),a("p",[e._v("它可以关注漏洞相似性方面的片段相似性，而不是整个函数的语法和语义相似性")]),e._v(" "),a("p",[e._v("首先，包含147个OpenSSL和 Linux CVE，它有助于描述漏洞狙击手。数据集在Github中可用。3•第二，使用与BiLSTM和Attention网络相结合的暹罗网络的检测模型。通过采用成对的漏洞函数和修补函数，该模型能够计算两个函数之间的相似性，从而检测漏洞。我们计划在本文发表后发布源代码。")]),e._v(" "),a("p",[e._v("•第三，对数据集进行系统实施和评估，以证明拟议模型的有效性。")]),e._v(" "),a("p",[e._v("具有相似语法或语义的两个函数并不一定能保证找到漏洞，因为漏洞片段只占整个漏洞函数的一小部分")]),e._v(" "),a("p",[e._v("Implementation details of VDSimilar")])])}),[],!1,null,null,null);a.default=i.exports}}]);